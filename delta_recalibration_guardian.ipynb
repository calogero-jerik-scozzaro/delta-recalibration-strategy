{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_global_perplexities(base_dir, author, epochs, transcription_type, batch_size):\n",
    "    subj_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    subj_dirs.sort()\n",
    "    \n",
    "    dict_ppl = {e: {} for e in epochs}   # epoch → {subj_dir: ppl}\n",
    "\n",
    "    for subj_dir in subj_dirs:\n",
    "        patient_path = os.path.join(base_dir, subj_dir)\n",
    "        for e in epochs:\n",
    "            file_name = f\"{subj_dir}_modello_{author}_{transcription_type}_{batch_size}b_{e}ep_global_ppl_score.txt\"\n",
    "            try:\n",
    "                with open(os.path.join(patient_path, file_name), 'r') as f:\n",
    "                    perplexity = float(f.read().strip())\n",
    "                    dict_ppl[e][subj_dir] = perplexity\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "    return dict_ppl\n",
    "\n",
    "def load_global_perplexities_only_same_class(base_dir, author, epochs, transcription_type, batch_size):\n",
    "    subj_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    subj_dirs.sort()\n",
    "    \n",
    "    dict_ppl = {e: {} for e in epochs}   # epoch → {subj_dir: ppl}\n",
    "\n",
    "    for subj_dir in subj_dirs:\n",
    "        if (subj_dir == author):\n",
    "            patient_path = os.path.join(base_dir, subj_dir)\n",
    "            for e in epochs:\n",
    "                file_name = f\"{subj_dir}_modello_{author}_{transcription_type}_{batch_size}b_{e}ep_global_ppl_score.txt\"\n",
    "                try:\n",
    "                    with open(os.path.join(patient_path, file_name), 'r') as f:\n",
    "                        perplexity = float(f.read().strip())\n",
    "                        dict_ppl[e][subj_dir] = perplexity\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "\n",
    "    return dict_ppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_global(window, epochs, dataset, transcription_type, batch_size, class_author_1, class_author_2):\n",
    "    dict_author_1 = {e: {} for e in epochs}\n",
    "    dict_author_2 = {e: {} for e in epochs}\n",
    "\n",
    "    # for each author\n",
    "    for auth in [class_author_1, class_author_2]:\n",
    "        base_dir = f\"guardian/{dataset}_w{window}_l0/dev/{auth}/{transcription_type}\"\n",
    "\n",
    "        # ppl obtained with model author_1\n",
    "        dict_tmp_1 = load_global_perplexities(base_dir, class_author_1, epochs, transcription_type, batch_size)\n",
    "        for e in epochs:\n",
    "            dict_author_1[e].update(dict_tmp_1[e])\n",
    "\n",
    "        # ppl obtained with model author_2\n",
    "        dict_tmp_2 = load_global_perplexities(base_dir, class_author_2, epochs, transcription_type, batch_size)\n",
    "        for e in epochs:\n",
    "            dict_author_2[e].update(dict_tmp_2[e])\n",
    "\n",
    "    # Calculate means\n",
    "    mean_author_1 = [np.nanmean(list(dict_author_1[e].values())) for e in epochs]\n",
    "    mean_author_2 = [np.nanmean(list(dict_author_2[e].values())) for e in epochs]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, mean_author_1, marker='o', label=f'{class_author_1} model on both authors', color='red')\n",
    "    plt.plot(epochs, mean_author_2, marker='o', label=f'{class_author_2} model on both authors', color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    for means, color in zip([mean_author_1, mean_author_2], ['red', 'blue']):\n",
    "        for x, y in zip(epochs, means):\n",
    "            plt.text(x, y + 0.02, f\"{y:.2f}\", ha='center', color=color)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # check if fig/{dataset} directory exists, if not create it\n",
    "    if not os.path.exists(\"fig/guardian\"):\n",
    "        os.makedirs(\"fig/guardian\")\n",
    "    \n",
    "    plot_name = f\"fig/guardian/perplexity_evolution_dev_set_fold_{class_author_1}_{class_author_2}.png\"\n",
    "    plt.savefig(plot_name, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # matrix of differences\n",
    "    matrix = np.empty((len(epochs), len(epochs)))\n",
    "    for i, ad_epoch in enumerate(epochs):\n",
    "        for j, cn_epoch in enumerate(epochs):\n",
    "            ad_val = mean_author_1[i]\n",
    "            cn_val = mean_author_2[j]\n",
    "            if not np.isnan(ad_val) and not np.isnan(cn_val):\n",
    "                matrix[i, j] = ad_val - cn_val\n",
    "            else:\n",
    "                matrix[i, j] = np.nan\n",
    "\n",
    "    matrix_df = pd.DataFrame(matrix, index=epochs, columns=epochs)\n",
    "    # check if matrices directory exists, if not create it\n",
    "    if not os.path.exists(\"guardian/matrices\"):\n",
    "        os.makedirs(\"guardian/matrices\")\n",
    "        \n",
    "    matrix_file = f\"guardian/matrices/matrix_diff_{class_author_1}_{class_author_2}.csv\"\n",
    "    matrix_df.to_csv(matrix_file, index=True, header=True)\n",
    "\n",
    "    mean_ad_df = pd.DataFrame(mean_author_1, index=epochs, columns=['Mean PPL'])\n",
    "    mean_cn_df = pd.DataFrame(mean_author_2, index=epochs, columns=['Mean PPL'])\n",
    "    mean_ad_file = f\"guardian/matrices/mean_{class_author_1}.csv\"\n",
    "    mean_cn_file = f\"guardian/matrices/mean_{class_author_2}.csv\"\n",
    "    mean_ad_df.to_csv(mean_ad_file, index=True, header=True)\n",
    "    mean_cn_df.to_csv(mean_cn_file, index=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_global_same_class(window, epochs, dataset, transcription_type, batch_size, class_author_1, class_author_2):\n",
    "    dict_author_1 = {e: {} for e in epochs}\n",
    "    dict_author_2 = {e: {} for e in epochs}\n",
    "\n",
    "    # for each author (data source)\n",
    "\n",
    "    base_dir = f\"guardian/{dataset}_w{window}_l0/dev/{class_author_1}/{transcription_type}\"\n",
    "    # ppl obtained with model author_1\n",
    "    dict_tmp_1 = load_global_perplexities(base_dir, class_author_1, epochs, transcription_type, batch_size)\n",
    "    for e in epochs:\n",
    "        dict_author_1[e].update(dict_tmp_1[e])\n",
    "\n",
    "    # ppl obtained with model author_2\n",
    "    base_dir = f\"guardian/{dataset}_w{window}_l0/dev/{class_author_2}/{transcription_type}\"\n",
    "    dict_tmp_2 = load_global_perplexities(base_dir, class_author_2, epochs, transcription_type, batch_size)\n",
    "    for e in epochs:\n",
    "        dict_author_2[e].update(dict_tmp_2[e])\n",
    "\n",
    "    # Calculate means\n",
    "    mean_author_1 = [np.nanmean(list(dict_author_1[e].values())) for e in epochs]\n",
    "    mean_author_2 = [np.nanmean(list(dict_author_2[e].values())) for e in epochs]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, mean_author_1, marker='o', label=f'{class_author_1} model on their texts', color='red')\n",
    "    plt.plot(epochs, mean_author_2, marker='o', label=f'{class_author_2} model on their texts', color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    for means, color in zip([mean_author_1, mean_author_2], ['red', 'blue']):\n",
    "        for x, y in zip(epochs, means):\n",
    "            plt.text(x, y + 0.02, f\"{y:.2f}\", ha='center', color=color)\n",
    "    plt.tight_layout()\n",
    "    plot_name = f\"fig/guardian/same_class_perplexity_evolution_dev_set_fold_{class_author_1}_{class_author_2}.png\"\n",
    "    plt.savefig(plot_name, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Matrix of differences\n",
    "    matrix = np.empty((len(epochs), len(epochs)))\n",
    "    for i, ad_epoch in enumerate(epochs):\n",
    "        for j, cn_epoch in enumerate(epochs):\n",
    "            ad_val = mean_author_1[i]\n",
    "            cn_val = mean_author_2[j]\n",
    "            if not np.isnan(ad_val) and not np.isnan(cn_val):\n",
    "                matrix[i, j] = ad_val - cn_val\n",
    "            else:\n",
    "                matrix[i, j] = np.nan\n",
    "\n",
    "    matrix_df = pd.DataFrame(matrix, index=epochs, columns=epochs)\n",
    "    matrix_file = f\"guardian/matrices/same_class_matrix_diff_{class_author_1}_{class_author_2}.csv\"\n",
    "    matrix_df.to_csv(matrix_file, index=True, header=True)\n",
    "\n",
    "\n",
    "    mean_ad_df = pd.DataFrame(mean_author_1, index=epochs, columns=['Mean PPL'])\n",
    "    mean_cn_df = pd.DataFrame(mean_author_2, index=epochs, columns=['Mean PPL'])\n",
    "    mean_ad_file = f\"guardian/matrices/same_class_mean_{class_author_1}.csv\"\n",
    "    mean_cn_file = f\"guardian/matrices/same_class_mean_{class_author_2}.csv\"\n",
    "    mean_ad_df.to_csv(mean_ad_file, index=True, header=True)\n",
    "    mean_cn_df.to_csv(mean_cn_file, index=True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_classification_test(base_dir_test, epochs, w, dataset, transcription_type, batch_size, author_1, author_2, list_authors_20):\n",
    "    dict_author_1 = load_global_perplexities(base_dir_test, author_1, epochs, transcription_type, batch_size)\n",
    "    dict_author_2 = load_global_perplexities(base_dir_test, author_2, epochs, transcription_type, batch_size)\n",
    "    \n",
    "    folders = [f for f in os.listdir(base_dir_test) if os.path.isdir(os.path.join(base_dir_test, f))]\n",
    "    folders = [f for f in folders if f.startswith(author_1) or f.startswith(author_2)]\n",
    "    folders.sort()\n",
    "\n",
    "    labels = {f: f.split(\"_\")[0] for f in folders}\n",
    "\n",
    "    results = []\n",
    "    \n",
    "\n",
    "    for i, author_1_epoch in enumerate(epochs):\n",
    "        for j, author_2_epoch in enumerate(epochs):\n",
    "            errors = 0\n",
    "            total = 0\n",
    "            list_errors_ids = []\n",
    "            count_errors_on_authors_20 = 0\n",
    "\n",
    "            for subj_name in folders:\n",
    "                subj_label = labels[subj_name]\n",
    "\n",
    "                ppl_author_1 = dict_author_1[author_1_epoch].get(subj_name)\n",
    "                ppl_author_2 = dict_author_2[author_2_epoch].get(subj_name)\n",
    "\n",
    "                if ppl_author_1 is None or ppl_author_2 is None:\n",
    "                    continue # Skip if perplexity is missing\n",
    "\n",
    "                if ppl_author_1 - ppl_author_2 > 0:\n",
    "                    pred = author_2\n",
    "                else:\n",
    "                    pred = author_1\n",
    "\n",
    "                if pred != subj_label:\n",
    "                    errors += 1\n",
    "                    list_errors_ids.append(subj_name)\n",
    "                    \n",
    "                    if subj_label in list_authors_20:\n",
    "                        count_errors_on_authors_20 += 1\n",
    "\n",
    "                total += 1\n",
    "\n",
    "            acc = 1 - (errors / total) if total else 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            results.append({\n",
    "                f'{author_1}_epoch': author_1_epoch,\n",
    "                f'{author_2}_epoch': author_2_epoch,\n",
    "                'total': total,\n",
    "                'errors': errors,\n",
    "                'accuracy': acc,\n",
    "                'list_errors_ids': list_errors_ids,\n",
    "                'count_errors_on_authors_20': count_errors_on_authors_20\n",
    "            })\n",
    "            \n",
    "    results_df = pd.DataFrame(results)\n",
    "    if not os.path.exists(\"guardian/results\"):\n",
    "        os.makedirs(\"guardian/results\")\n",
    "    results_df.to_csv(f\"guardian/results/classification_results_{author_1}_{author_2}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_test(base_dir_test, epochs, w, dataset, transcription_type, batch_size, author_1, author_2, list_authors_20):\n",
    "    dict_author_1 = load_global_perplexities(base_dir_test, author_1, epochs, transcription_type, batch_size)\n",
    "    dict_author_2 = load_global_perplexities(base_dir_test, author_2, epochs, transcription_type, batch_size)\n",
    "    \n",
    "    folders = [f for f in os.listdir(base_dir_test) if os.path.isdir(os.path.join(base_dir_test, f))]\n",
    "    folders = [f for f in folders if f.startswith(author_1) or f.startswith(author_2)]\n",
    "    folders.sort()\n",
    "\n",
    "    labels = {f: f.split(\"_\")[0] for f in folders}\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for i, author_1_epoch in enumerate(epochs):\n",
    "        for j, author_2_epoch in enumerate(epochs):\n",
    "            errors = 0\n",
    "            total = 0\n",
    "            list_errors_ids = []\n",
    "            count_errors_on_authors_20 = 0\n",
    "\n",
    "            # confusion counters\n",
    "            count_a1_pred_a1 = 0\n",
    "            count_a1_pred_a2 = 0\n",
    "            count_a2_pred_a1 = 0\n",
    "            count_a2_pred_a2 = 0\n",
    "\n",
    "            for subj_name in folders:\n",
    "                subj_label = labels[subj_name]\n",
    "\n",
    "                ppl_author_1 = dict_author_1[author_1_epoch].get(subj_name)\n",
    "                ppl_author_2 = dict_author_2[author_2_epoch].get(subj_name)\n",
    "\n",
    "                if ppl_author_1 is None or ppl_author_2 is None:\n",
    "                    continue  # skip if missing data\n",
    "\n",
    "                if ppl_author_1 - ppl_author_2 > 0:\n",
    "                    pred = author_2\n",
    "                else:\n",
    "                    pred = author_1\n",
    "\n",
    "                if pred != subj_label:\n",
    "                    errors += 1\n",
    "                    list_errors_ids.append(subj_name)\n",
    "                    \n",
    "                    if subj_label in list_authors_20:\n",
    "                        count_errors_on_authors_20 += 1\n",
    "\n",
    "                # update confusion matrix\n",
    "                if subj_label == author_1:\n",
    "                    if pred == author_1:\n",
    "                        count_a1_pred_a1 += 1\n",
    "                    else:\n",
    "                        count_a1_pred_a2 += 1\n",
    "                else:  # subj_label == author_2\n",
    "                    if pred == author_2:\n",
    "                        count_a2_pred_a2 += 1\n",
    "                    else:\n",
    "                        count_a2_pred_a1 += 1\n",
    "\n",
    "                total += 1\n",
    "\n",
    "            acc = 1 - (errors / total) if total else 0\n",
    "            \n",
    "            # --- F1 score computation ---\n",
    "            f1_per_class = {}\n",
    "\n",
    "            # For author_1\n",
    "            precision_a1 = count_a1_pred_a1 / (count_a1_pred_a1 + count_a2_pred_a1) if (count_a1_pred_a1 + count_a2_pred_a1) > 0 else 0\n",
    "            recall_a1    = count_a1_pred_a1 / (count_a1_pred_a1 + count_a1_pred_a2) if (count_a1_pred_a1 + count_a1_pred_a2) > 0 else 0\n",
    "            f1_per_class[author_1] = 2 * (precision_a1 * recall_a1) / (precision_a1 + recall_a1) if (precision_a1 + recall_a1) > 0 else 0\n",
    "\n",
    "            # For author_2\n",
    "            precision_a2 = count_a2_pred_a2 / (count_a2_pred_a2 + count_a1_pred_a2) if (count_a2_pred_a2 + count_a1_pred_a2) > 0 else 0\n",
    "            recall_a2    = count_a2_pred_a2 / (count_a2_pred_a2 + count_a2_pred_a1) if (count_a2_pred_a2 + count_a2_pred_a1) > 0 else 0\n",
    "            f1_per_class[author_2] = 2 * (precision_a2 * recall_a2) / (precision_a2 + recall_a2) if (precision_a2 + recall_a2) > 0 else 0\n",
    "\n",
    "            macro_f1 = (f1_per_class[author_1] + f1_per_class[author_2]) / 2\n",
    "\n",
    "            results.append({\n",
    "                f'{author_1}_epoch': author_1_epoch,\n",
    "                f'{author_2}_epoch': author_2_epoch,\n",
    "                'total': total,\n",
    "                'errors': errors,\n",
    "                'accuracy': acc,\n",
    "                'list_errors_ids': list_errors_ids,\n",
    "                'count_errors_on_authors_20': count_errors_on_authors_20,\n",
    "                f'f1_{author_1}': f1_per_class[author_1],\n",
    "                f'f1_{author_2}': f1_per_class[author_2],\n",
    "                'macro_f1': macro_f1\n",
    "            })\n",
    "            \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # ensure result folder exists\n",
    "    if not os.path.exists(f\"guardian/results\"):\n",
    "        os.makedirs(f\"guardian/results\")\n",
    "        \n",
    "    results_df.to_csv(f\"guardian/results/classification_results_{author_1}_{author_2}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def print_correlation_ppl_valid_classific_test_not_overfitting(epochs, dataset, author_1, author_2):\n",
    "    try:\n",
    "        # Load classification results\n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load delta matrix\n",
    "        matrix_file = f\"guardian/matrices/matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return\n",
    "\n",
    "    # find epoch with minimum mean perplexity for each author\n",
    "    min_a1_idx = mean_author_1_df.values.argmin()\n",
    "    min_a2_idx = mean_author_2_df.values.argmin()\n",
    "    min_a1_val = epochs[min_a1_idx]\n",
    "    min_a2_val = epochs[min_a2_idx]\n",
    "\n",
    "    # filter results to only include epochs up to the minimum perplexity epoch\n",
    "    results_df = results_df[\n",
    "        (results_df[f'{author_1}_epoch'] <= min_a1_val) &\n",
    "        (results_df[f'{author_2}_epoch'] <= min_a2_val)\n",
    "    ]\n",
    "\n",
    "    # Calcola deltas\n",
    "    deltas = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        try:\n",
    "            i = epochs.index(row[f'{author_1}_epoch'])\n",
    "            j = epochs.index(row[f'{author_2}_epoch'])\n",
    "            deltas.append(abs(matrix[i, j]))\n",
    "        except Exception:\n",
    "            deltas.append(np.nan)\n",
    "\n",
    "    results_df['delta'] = deltas\n",
    "    results_df.dropna(subset=['accuracy', 'delta'], inplace=True)\n",
    "\n",
    "    if len(results_df) < 2:\n",
    "        print(f\"⚠️ Not enough valid data to compute correlation authors={author_1} vs {author_2}\")\n",
    "        return\n",
    "\n",
    "    # Correlations\n",
    "    pearson_corr, pearson_p = pearsonr(results_df['delta'], results_df['accuracy'])\n",
    "    spearman_corr, spearman_p = spearmanr(results_df['delta'], results_df['accuracy'])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(results_df['delta'], results_df['accuracy'], alpha=0.7)\n",
    "\n",
    "    z = np.polyfit(results_df['delta'], results_df['accuracy'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_vals = np.sort(results_df['delta'])\n",
    "    plt.plot(x_vals, p(x_vals), \"r--\", label=f\"y={z[0]:.4f}x + {z[1]:.4f}\")\n",
    "    \n",
    "    # Text annotation\n",
    "    plt.text(0.05, 0.05,\n",
    "             f\"Pearson r: {pearson_corr:.4f} (p={pearson_p:.4e})\\n\"\n",
    "             f\"Spearman r: {spearman_corr:.4f} (p={spearman_p:.4e})\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.xlabel(f\"Delta Perplexity ({author_1} vs {author_2})\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.yticks(np.arange(0, 1.01, 0.1))\n",
    "    plt.xlim(left=0)\n",
    "    max_val = results_df['delta'].max()\n",
    "    plt.xticks(np.arange(0, max_val + 2, 2))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plot_path = f\"fig/guardian/not_overfitting_accuracy_vs_delta_{author_1}_{author_2}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    # print(f\"✅ Plot saved: {plot_path}\")\n",
    "    # print(f\"📊 Pearson correlation: {pearson_corr:.4f} (p={pearson_p:.4e})\")\n",
    "    # print(f\"📈 Spearman correlation: {spearman_corr:.4f} (p={spearman_p:.4e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def print_correlation_ppl_valid_classific_test_not_overfitting_same_class(epochs, dataset, author_1, author_2):\n",
    "    try:\n",
    "        # Load classification results\n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load delta matrix\n",
    "        matrix_file = f\"guardian/matrices/same_class_matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/same_class_mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/same_class_mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Find epoch with minimum mean perplexity for each author\n",
    "    min_a1_idx = mean_author_1_df.values.argmin()\n",
    "    min_a2_idx = mean_author_2_df.values.argmin()\n",
    "    min_a1_val = epochs[min_a1_idx]\n",
    "    min_a2_val = epochs[min_a2_idx]\n",
    "\n",
    "    # Filter results to only include epochs up to the minimum perplexity epoch\n",
    "    results_df = results_df[\n",
    "        (results_df[f'{author_1}_epoch'] <= min_a1_val) &\n",
    "        (results_df[f'{author_2}_epoch'] <= min_a2_val)\n",
    "    ]\n",
    "\n",
    "    # Calcola deltas\n",
    "    deltas = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        try:\n",
    "            i = epochs.index(row[f'{author_1}_epoch'])\n",
    "            j = epochs.index(row[f'{author_2}_epoch'])\n",
    "            deltas.append(abs(matrix[i, j]))\n",
    "        except Exception:\n",
    "            deltas.append(np.nan)\n",
    "\n",
    "    results_df['delta'] = deltas\n",
    "    results_df.dropna(subset=['accuracy', 'delta'], inplace=True)\n",
    "\n",
    "    if len(results_df) < 2:\n",
    "        print(f\"⚠️ Not enough valid data to compute correlation authors={author_1} vs {author_2}\")\n",
    "        return\n",
    "\n",
    "    # Correlations\n",
    "    pearson_corr, pearson_p = pearsonr(results_df['delta'], results_df['accuracy'])\n",
    "    spearman_corr, spearman_p = spearmanr(results_df['delta'], results_df['accuracy'])\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(results_df['delta'], results_df['accuracy'], alpha=0.7)\n",
    "\n",
    "    z = np.polyfit(results_df['delta'], results_df['accuracy'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_vals = np.sort(results_df['delta'])\n",
    "    plt.plot(x_vals, p(x_vals), \"r--\", label=f\"y={z[0]:.4f}x + {z[1]:.4f}\")\n",
    "    \n",
    "    # Text annotation\n",
    "    plt.text(0.05, 0.05,\n",
    "             f\"Pearson r: {pearson_corr:.4f} (p={pearson_p:.4e})\\n\"\n",
    "             f\"Spearman r: {spearman_corr:.4f} (p={spearman_p:.4e})\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.xlabel(f\"Delta Perplexity ({author_1} vs {author_2})\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.yticks(np.arange(0, 1.01, 0.1))\n",
    "    plt.xlim(left=0)\n",
    "    max_val = results_df['delta'].max()\n",
    "    plt.xticks(np.arange(0, max_val + 2, 2))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plot_path = f\"fig/guardian/same_class_not_overfitting_accuracy_vs_delta_{author_1}_{author_2}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    # print(f\"✅ Plot saved: {plot_path}\")\n",
    "    # print(f\"📊 Pearson correlation: {pearson_corr:.4f} (p={pearson_p:.4e})\")\n",
    "    # print(f\"📈 Spearman correlation: {spearman_corr:.4f} (p={spearman_p:.4e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_not_overfitting_baseline(epochs, dataset, author_1, author_2):\n",
    "    try:\n",
    "        # Load results\n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load matrix as NumPy array\n",
    "        matrix_file = f\"guardian/matrices/matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Find epoch with minimum mean perplexity for each author\n",
    "    min_a1_idx = mean_author_1_df.values.argmin()\n",
    "    min_a2_idx = mean_author_2_df.values.argmin()\n",
    "    \n",
    "    #print(f\"Min {author_1} PPL Epoch: {epochs[min_a1_idx]}, Min {author_2} PPL Epoch: {epochs[min_a2_idx]}\")\n",
    "    min_a1_val = epochs[min_a1_idx]\n",
    "    min_a2_val = epochs[min_a2_idx]\n",
    "\n",
    "    # Filter results to only include the specific epoch combination\n",
    "    results_df = results_df[\n",
    "        (results_df[f'{author_1}_epoch'] == min_a1_val) &\n",
    "        (results_df[f'{author_2}_epoch'] == min_a2_val)\n",
    "    ]\n",
    "    accuracy = results_df.iloc[0]['accuracy']\n",
    "    count_errors_on_authors_20 = results_df.iloc[0]['count_errors_on_authors_20']\n",
    "    count_errors = results_df.iloc[0]['errors']\n",
    "    f1 = results_df.iloc[0]['macro_f1']\n",
    "    #print(f\"Baseline BC Accuracy ({author_1} epoch={min_a1_val}, {author_2} epoch={min_a2_val}) ---> {accuracy:.4f}\")\n",
    "\n",
    "    return accuracy, count_errors, count_errors_on_authors_20, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def save_delta_classification_results(epochs, dataset, author_1, author_2):\n",
    "    try:\n",
    "        # Load baseline results\n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load matrix\n",
    "        matrix_file = f\"guardian/matrices/matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load baseline data for {author_1} vs {author_2}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Compute error bounds using epoch indices\n",
    "    deltas = []\n",
    "    for _, row in results_df.iterrows():\n",
    "        try:\n",
    "            i = epochs.index(row[f'{author_1}_epoch'])\n",
    "            j = epochs.index(row[f'{author_2}_epoch'])\n",
    "            deltas.append(abs(matrix[i, j]))\n",
    "        except Exception:\n",
    "            deltas.append(np.nan)\n",
    "\n",
    "    results_df['delta'] = deltas\n",
    "    results_df.dropna(subset=['accuracy', 'delta'], inplace=True)\n",
    "\n",
    "    if len(results_df) < 2:\n",
    "        print(f\"⚠️ Not enough valid data to compute correlation for {author_1} vs {author_2}\")\n",
    "        return None\n",
    "\n",
    "    # Correlation computations\n",
    "    pearson_corr, pearson_p = pearsonr(results_df['delta'], results_df['accuracy'])\n",
    "    spearman_corr, spearman_p = spearmanr(results_df['delta'], results_df['accuracy'])\n",
    "\n",
    "    # Save the delta results\n",
    "    delta_results_file = f\"guardian/results/with_delta_classification_results_{author_1}_{author_2}.csv\"\n",
    "    results_df.to_csv(delta_results_file, index=False)\n",
    "    print(f\"✅ Saved delta results CSV: {delta_results_file}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(results_df['delta'], results_df['accuracy'], alpha=0.7)\n",
    "\n",
    "    # Regression line\n",
    "    z = np.polyfit(results_df['delta'], results_df['accuracy'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_vals = np.sort(results_df['delta'])\n",
    "    plt.plot(x_vals, p(x_vals), \"r--\", label=f\"y={z[0]:.4f}x + {z[1]:.4f}\")\n",
    "\n",
    "    plt.text(0.05, 0.05,\n",
    "             f\"Pearson r: {pearson_corr:.4f} (p={pearson_p:.4e})\\n\"\n",
    "             f\"Spearman r: {spearman_corr:.4f} (p={spearman_p:.4e})\",\n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.xlabel(\"Delta Perplexity\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"Accuracy vs. Delta Perplexity – {author_1} vs {author_2}\")\n",
    "    plt.yticks(np.arange(0, 1.01, 0.05))\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save plot\n",
    "    plot_path = f\"fig/guardian/accuracy_vs_delta_{author_1}_{author_2}.png\"\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    return delta_results_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_not_overfitting_delta(epochs, dataset, author_1, author_2):\n",
    "    try:\n",
    "        # Load results (delta classification)\n",
    "        results_file = f\"guardian/results/with_delta_classification_results_{author_1}_{author_2}.csv\"\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load matrix\n",
    "        matrix_file = f\"guardian/matrices/matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/mean_{author_2}.csv\"\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load delta data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Best epochs (minimum PPL for each author)\n",
    "    min_a1_idx = mean_author_1_df.values.argmin()\n",
    "    min_a2_idx = mean_author_2_df.values.argmin()\n",
    "    min_a1_val = epochs[min_a1_idx]\n",
    "    min_a2_val = epochs[min_a2_idx]\n",
    "\n",
    "    # Filter: only keep configs within optimal epochs\n",
    "    results_df = results_df[\n",
    "        (results_df[f'{author_1}_epoch'] <= min_a1_val) &\n",
    "        (results_df[f'{author_2}_epoch'] <= min_a2_val)\n",
    "    ]\n",
    "    results_df.dropna(subset=['accuracy'], inplace=True)\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(f\"⚠️ No valid configurations found for {author_1} vs {author_2} (delta).\")\n",
    "        return None\n",
    "\n",
    "    # Pick configuration with minimum error bound\n",
    "    if 'delta' in results_df.columns:\n",
    "        min_lower_bound = results_df['delta'].min()\n",
    "        best_config = results_df[results_df['delta'] == min_lower_bound].iloc[0]\n",
    "    else:\n",
    "        # Fallback: choose max accuracy if delta not present\n",
    "        best_config = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "\n",
    "    accuracy = best_config['accuracy']\n",
    "    count_errors = best_config['errors']\n",
    "    count_errors_on_authors_20 = best_config['count_errors_on_authors_20']\n",
    "    f1 = best_config['macro_f1']\n",
    "\n",
    "    # print(f\"Delta SC Accuracy ({author_1} epoch={best_config[f'{author_1}_epoch']}, \"\n",
    "    #       f\"{author_2} epoch={best_config[f'{author_2}_epoch']}) ---> {accuracy:.4f}\")\n",
    "\n",
    "    # if 'ad_predicted_cn' in best_config and 'cn_predicted_ad' in best_config:\n",
    "    #     print(f\"{author_1} predicted {author_2}: {best_config['ad_predicted_cn']}, \"\n",
    "    #           f\"{author_2} predicted {author_1}: {best_config['cn_predicted_ad']}\")\n",
    "\n",
    "    return accuracy, count_errors, count_errors_on_authors_20, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_not_overfitting_delta_same_class(epochs, dataset, author_1, author_2):\n",
    "    try:\n",
    "        # Load results (delta classification)\n",
    "        results_file = f\"guardian/results/with_delta_classification_results_{author_1}_{author_2}.csv\"\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load matrix\n",
    "        matrix_file = f\"guardian/matrices/same_class_matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/same_class_mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/same_class_mean_{author_2}.csv\"\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load delta data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Best epochs (minimum PPL for each author)\n",
    "    min_a1_idx = mean_author_1_df.values.argmin()\n",
    "    min_a2_idx = mean_author_2_df.values.argmin()\n",
    "    min_a1_val = epochs[min_a1_idx]\n",
    "    min_a2_val = epochs[min_a2_idx]\n",
    "\n",
    "    # Filter: only keep configs within optimal epochs\n",
    "    results_df = results_df[\n",
    "        (results_df[f'{author_1}_epoch'] <= min_a1_val) &\n",
    "        (results_df[f'{author_2}_epoch'] <= min_a2_val)\n",
    "    ]\n",
    "    results_df.dropna(subset=['accuracy'], inplace=True)\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(f\"⚠️ No valid configurations found for {author_1} vs {author_2} (delta).\")\n",
    "        return None\n",
    "\n",
    "    # Pick configuration with minimum error bound\n",
    "    if 'delta' in results_df.columns:\n",
    "        min_lower_bound = results_df['delta'].min()\n",
    "        best_config = results_df[results_df['delta'] == min_lower_bound].iloc[0]\n",
    "    else:\n",
    "        # Fallback: choose max accuracy if delta not present\n",
    "        best_config = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "\n",
    "    accuracy = best_config['accuracy']\n",
    "    errors = best_config['errors']\n",
    "    count_errors_on_authors_20 = best_config['count_errors_on_authors_20']\n",
    "    f1 = best_config['macro_f1']\n",
    "\n",
    "    # print(f\"Delta BC Accuracy ({author_1} epoch={best_config[f'{author_1}_epoch']}, \"\n",
    "    #       f\"{author_2} epoch={best_config[f'{author_2}_epoch']}) ---> {accuracy:.4f}\")\n",
    "\n",
    "    # if 'ad_predicted_cn' in best_config and 'cn_predicted_ad' in best_config:\n",
    "    #     print(f\"{author_1} predicted {author_2}: {best_config['ad_predicted_cn']}, \"\n",
    "    #           f\"{author_2} predicted {author_1}: {best_config['cn_predicted_ad']}\")\n",
    "\n",
    "    return accuracy, errors, count_errors_on_authors_20, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_not_overfitting_baseline_same_class(epochs, dataset, author_1, author_2):\n",
    "    try:\n",
    "        # Load results\n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load matrix as NumPy array\n",
    "        matrix_file = f\"guardian/matrices/same_class_matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/same_class_mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/same_class_mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Find epoch with minimum mean perplexity for each author\n",
    "    min_a1_idx = mean_author_1_df.values.argmin()\n",
    "    min_a2_idx = mean_author_2_df.values.argmin()\n",
    "    \n",
    "    #print(f\"Min {author_1} PPL Epoch: {epochs[min_a1_idx]}, Min {author_2} PPL Epoch: {epochs[min_a2_idx]}\")\n",
    "    min_a1_val = epochs[min_a1_idx]\n",
    "    min_a2_val = epochs[min_a2_idx]\n",
    "\n",
    "    # Filtra i risultati entro le epoche ottimali\n",
    "    results_df = results_df[\n",
    "        (results_df[f'{author_1}_epoch'] == min_a1_val) &\n",
    "        (results_df[f'{author_2}_epoch'] == min_a2_val)\n",
    "    ]\n",
    "    accuracy = results_df.iloc[0]['accuracy']\n",
    "    count_errors_on_authors_20 = results_df.iloc[0]['count_errors_on_authors_20']\n",
    "    count_errors = results_df.iloc[0]['errors']\n",
    "    #print(f\"Baseline SC Accuracy ({author_1} epoch={min_a1_val}, {author_2} epoch={min_a2_val}) ---> {accuracy:.4f}\")\n",
    "    f1 = results_df.iloc[0]['macro_f1']\n",
    "\n",
    "    return accuracy, count_errors, count_errors_on_authors_20, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_oracle(epochs, dataset, author_1, author_2):\n",
    "    try:\n",
    "        # Load results\n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load matrix as NumPy array\n",
    "        matrix_file = f\"guardian/matrices/matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # sorter by accuracy descending\n",
    "    results_df = results_df.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "    # get the best accuracy\n",
    "    accuracy = results_df.iloc[0]['accuracy']\n",
    "    f1 = results_df.iloc[0]['macro_f1']\n",
    "\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"guardian_processed\"\n",
    "transcription_type = \"manual\"\n",
    "batch_size = 12\n",
    "window = 20  # You can adjust this list as needed\n",
    "epochs = list(range(1, 16, 1))\n",
    "list_of_author = ['catherinebennett', 'georgemonbiot', 'hugoyoung', 'jonathanfreedland',\n",
    "                  \"nickcohen\",\"zoewilliams\", \"pollytoynbee\",\"peterpreston\",  \"royhattersley\",\"simonhoggart\",\n",
    "                  \"martinkettle\",\"maryriddell\", \"willhutton\"\n",
    "                  ]\n",
    "\n",
    "list_authors_20 = [\"nickcohen\",\"zoewilliams\", \"pollytoynbee\",\"peterpreston\", \"royhattersley\", \"simonhoggart\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}\"\n",
    "\n",
    "# for each pair of authors, do classification\n",
    "for i in range(len(list_of_author)):\n",
    "    for j in range(i + 1, len(list_of_author)):\n",
    "        author_1 = list_of_author[i]\n",
    "        author_2 = list_of_author[j]\n",
    "        \n",
    "        try:\n",
    "            process_and_plot_global(window, epochs, dataset, transcription_type, batch_size, author_1, author_2)\n",
    "            process_and_plot_global_same_class(window, epochs, dataset, transcription_type, batch_size, author_1, author_2)\n",
    "            print(f\"✅ Global perplexity processed and plotted for authors: {author_1} vs {author_2}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during global perplexity processing for authors: {author_1} vs {author_2}: {e}\")\n",
    "            \n",
    "        try:\n",
    "            classification_test(base_dir_test, epochs, window, dataset, transcription_type, batch_size, author_1, author_2, list_authors_20)\n",
    "            print(f\"✅ Classification test completed for authors: {author_1} vs {author_2}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during classification for authors: {author_1} vs {author_2}: {e}\")\n",
    "            \n",
    "        try:\n",
    "            save_delta_classification_results(epochs, dataset, author_1, author_2)\n",
    "            print_correlation_ppl_valid_classific_test_not_overfitting(epochs, dataset, author_1, author_2)\n",
    "            print_correlation_ppl_valid_classific_test_not_overfitting_same_class(epochs, dataset, author_1, author_2)\n",
    "            print(f\"✅ Correlation analysis completed for authors: {author_1} vs {author_2}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during correlation analysis for authors: {author_1} vs {author_2}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_accuracy_baseline_bc = []\n",
    "list_accuracy_delta_bc = []\n",
    "list_accuracy_baseline_sc = []\n",
    "list_accuracy_delta_sc = []\n",
    "list_accuracy_oracle = []\n",
    "\n",
    "total_count_errors_baseline_sc = 0\n",
    "total_count_errors_20_baseline_sc = 0\n",
    "total_count_errors_baseline_bc = 0\n",
    "total_count_errors_20_baseline_bc = 0\n",
    "total_count_errors_delta_sc = 0\n",
    "total_count_errors_20_delta_sc = 0\n",
    "total_count_errors_delta_bc = 0\n",
    "total_count_errors_20_delta_bc = 0\n",
    "\n",
    "list_f1_baseline_bc = []\n",
    "list_f1_delta_bc = []\n",
    "list_f1_baseline_sc = []\n",
    "list_f1_delta_sc = []\n",
    "list_f1_oracle = []\n",
    "\n",
    "for i in range(len(list_of_author)):\n",
    "    for j in range(i + 1, len(list_of_author)):\n",
    "        author_1 = list_of_author[i]\n",
    "        author_2 = list_of_author[j]\n",
    "        \n",
    "        try:\n",
    "            acc, count_errors, count_errors_on_authors_20,f1 = print_accuracy_not_overfitting_baseline_same_class(epochs, dataset, author_1, author_2)\n",
    "            list_accuracy_baseline_sc.append(acc)\n",
    "            list_f1_baseline_sc.append(f1)\n",
    "            total_count_errors_baseline_sc += count_errors\n",
    "            total_count_errors_20_baseline_sc += count_errors_on_authors_20\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during same class average accuracy computation for authors: {author_1} vs {author_2}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            acc, count_errors, count_errors_on_authors_20,f1 = print_accuracy_not_overfitting_baseline(epochs, dataset, author_1, author_2)\n",
    "            list_accuracy_baseline_bc.append(acc)\n",
    "            list_f1_baseline_bc.append(f1)\n",
    "            total_count_errors_baseline_bc += count_errors\n",
    "            total_count_errors_20_baseline_bc += count_errors_on_authors_20\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during average accuracy computation for authors: {author_1} vs {author_2}: {e}\") \n",
    "            \n",
    "        try:\n",
    "            acc, count_errors, count_errors_on_authors_20,f1 = print_accuracy_not_overfitting_delta_same_class(epochs, dataset, author_1, author_2)\n",
    "            list_accuracy_delta_sc.append(acc)\n",
    "            list_f1_delta_sc.append(f1)\n",
    "            total_count_errors_delta_sc += count_errors\n",
    "            total_count_errors_20_delta_sc += count_errors_on_authors_20\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during delta same class accuracy computation for authors: {author_1} vs {author_2}: {e}\")\n",
    "            \n",
    "        try:\n",
    "            acc, count_errors, count_errors_on_authors_20, f1 = print_accuracy_not_overfitting_delta(epochs, dataset, author_1, author_2)\n",
    "            list_accuracy_delta_bc.append(acc)\n",
    "            list_f1_delta_bc.append(f1)\n",
    "            total_count_errors_delta_bc += count_errors\n",
    "            total_count_errors_20_delta_bc += count_errors_on_authors_20\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during delta accuracy computation for authors: {author_1} vs {author_2}: {e}\")\n",
    "            \n",
    "        try:\n",
    "            acc,f1 = print_accuracy_oracle(epochs, dataset, author_1, author_2)\n",
    "            list_accuracy_oracle.append(acc)\n",
    "            list_f1_oracle.append(f1)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during oracle accuracy computation for authors: {author_1} vs {author_2}: {e}\")\n",
    "            \n",
    "        #print(\"--------------------------------------------------\")\n",
    "            \n",
    "if list_accuracy_baseline_sc:\n",
    "    overall_avg_baseline_sc = np.nanmean(list_accuracy_baseline_sc)\n",
    "    std = np.nanstd(list_accuracy_baseline_sc)\n",
    "    print(f\"\\nOverall Average Baseline Same Class Accuracy: {overall_avg_baseline_sc:.4f}, std: {std:.4f}\")\n",
    "    overall_f1_baseline_sc = np.nanmean(list_f1_baseline_sc)\n",
    "    std_f1 = np.nanstd(list_f1_baseline_sc)\n",
    "    print(f\"Overall Average Baseline Same Class F1: {overall_f1_baseline_sc:.4f}, std: {std_f1:.4f}\")\n",
    "    \n",
    "    print(f\"Percentage of errors on authors_20 (SC): {total_count_errors_20_baseline_sc}/{total_count_errors_baseline_sc} = {total_count_errors_20_baseline_sc/total_count_errors_baseline_sc*100:.2f}%\")\n",
    "    \n",
    "print(\"---------------------------------------------------\")\n",
    "if list_accuracy_baseline_bc:\n",
    "    overall_avg_baseline = np.nanmean(list_accuracy_baseline_bc)\n",
    "    std = np.nanstd(list_accuracy_baseline_bc)\n",
    "    print(f\"Overall Average Baseline Both Class Accuracy: {overall_avg_baseline:.4f}, std: {std:.4f}\")\n",
    "    overall_f1_baseline_bc = np.nanmean(list_f1_baseline_bc)\n",
    "    std_f1 = np.nanstd(list_f1_baseline_bc)\n",
    "    print(f\"Overall Average Baseline Both Class F1: {overall_f1_baseline_bc:.4f}, std: {std_f1:.4f}\")\n",
    "    \n",
    "    print(f\"Percentage of errors on authors_20 (BC): {total_count_errors_20_baseline_bc}/{total_count_errors_baseline_bc} = {total_count_errors_20_baseline_bc/total_count_errors_baseline_bc*100:.2f}%\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "if list_accuracy_delta_sc:\n",
    "    overall_avg_delta_sc = np.nanmean(list_accuracy_delta_sc)\n",
    "    std = np.nanstd(list_accuracy_delta_sc)\n",
    "    print(f\"Overall Average Delta Same Class Accuracy: {overall_avg_delta_sc:.4f}, std: {std:.4f}\")\n",
    "    overall_f1_delta_sc = np.nanmean(list_f1_delta_sc)\n",
    "    std_f1 = np.nanstd(list_f1_delta_sc)\n",
    "    print(f\"Overall Average Delta Same Class F1: {overall_f1_delta_sc:.4f}, std: {std_f1:.4f}\")\n",
    "    print(f\"Percentage of errors on authors_20 (Delta SC): {total_count_errors_20_delta_sc}/{total_count_errors_delta_sc} = {total_count_errors_20_delta_sc/total_count_errors_delta_sc*100:.2f}%\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "if list_accuracy_delta_bc:\n",
    "    overall_avg_delta = np.nanmean(list_accuracy_delta_bc)\n",
    "    std = np.nanstd(list_accuracy_delta_bc)\n",
    "    print(f\"Overall Average Delta Both Class Accuracy: {overall_avg_delta:.4f}, std: {std:.4f}\")\n",
    "    overall_f1_delta_bc = np.nanmean(list_f1_delta_bc)\n",
    "    std_f1 = np.nanstd(list_f1_delta_bc)\n",
    "    print(f\"Overall Average Delta Both Class F1: {overall_f1_delta_bc:.4f}, std: {std_f1:.4f}\")\n",
    "    print(f\"Percentage of errors on authors_20 (Delta BC): {total_count_errors_20_delta_bc}/{total_count_errors_delta_bc} = {total_count_errors_20_delta_bc/total_count_errors_delta_bc*100:.2f}%\")\n",
    "    \n",
    "print(\"---------------------------------------------------\")\n",
    "if list_accuracy_oracle:\n",
    "    overall_avg_oracle = np.nanmean(list_accuracy_oracle)\n",
    "    std = np.nanstd(list_accuracy_oracle)\n",
    "    print(f\"Overall Average Oracle Accuracy: {overall_avg_oracle:.4f}, std: {std:.4f}\")\n",
    "    overall_f1_oracle = np.nanmean(list_f1_oracle)\n",
    "    std_f1 = np.nanstd(list_f1_oracle)\n",
    "    print(f\"Overall Average Oracle F1: {overall_f1_oracle:.4f}, std: {std_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_global_perplexities_all_bc(base_dir, author, epochs, transcription_type, batch_size):\n",
    "    subj_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    subj_dirs.sort()\n",
    "    \n",
    "    dict_ppl = {e: 0 for e in epochs}   # epoch → {subj_dir: ppl}\n",
    "\n",
    "    for ep in epochs:\n",
    "        ppl = 0\n",
    "        for subj_dir in subj_dirs:\n",
    "            patient_path = os.path.join(base_dir, subj_dir)\n",
    "            patient_path = os.path.join(patient_path, transcription_type)\n",
    "            list_of_subdirs = os.listdir(patient_path)\n",
    "            # remove .DS_Store if present\n",
    "            if '.DS_Store' in list_of_subdirs:\n",
    "                list_of_subdirs.remove('.DS_Store')\n",
    "            for subdir in list_of_subdirs:\n",
    "                patient_path = os.path.join(patient_path, subdir)\n",
    "                file_name = f\"{subdir}_modello_{author}_{transcription_type}_{batch_size}b_{ep}ep_global_ppl_score.txt\"\n",
    "                try:\n",
    "                    with open(os.path.join(patient_path, file_name), 'r') as f:\n",
    "                        perplexity = float(f.read().strip())\n",
    "                        ppl += perplexity\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "        dict_ppl[ep] = ppl / len(subj_dirs) if len(subj_dirs) > 0 else np.nan\n",
    "    return dict_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_global_perplexities_all_sc(base_dir, author, epochs, transcription_type, batch_size):\n",
    "    subj_dirs = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "    subj_dirs.sort()\n",
    "    \n",
    "    dict_ppl = {e: 0 for e in epochs}   # epoch → {subj_dir: ppl}\n",
    "\n",
    "    for ep in epochs:\n",
    "        ppl = 0\n",
    "        for subj_dir in subj_dirs:\n",
    "            if (subj_dir == author):\n",
    "                patient_path = os.path.join(base_dir, subj_dir)\n",
    "                patient_path = os.path.join(patient_path, transcription_type)\n",
    "                list_of_subdirs = os.listdir(patient_path)\n",
    "                # remove .DS_Store if present\n",
    "                if '.DS_Store' in list_of_subdirs:\n",
    "                    list_of_subdirs.remove('.DS_Store')\n",
    "                for subdir in list_of_subdirs:\n",
    "                    patient_path = os.path.join(patient_path, subdir)\n",
    "                    file_name = f\"{subdir}_modello_{author}_{transcription_type}_{batch_size}b_{ep}ep_global_ppl_score.txt\"\n",
    "                    try:\n",
    "                        with open(os.path.join(patient_path, file_name), 'r') as f:\n",
    "                            perplexity = float(f.read().strip())\n",
    "                            ppl += perplexity\n",
    "                    except FileNotFoundError:\n",
    "                        continue\n",
    "        dict_ppl[ep] = ppl / len(subj_dirs) if len(subj_dirs) > 0 else np.nan\n",
    "    return dict_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_vs_all_classify_subject_baseline(dataset: str,\n",
    "                                            window: int,\n",
    "                                            transcription_type: str,\n",
    "                                            batch_size: int,\n",
    "                                            epochs: list,\n",
    "                                            authors: list,\n",
    "                                            subj: str,\n",
    "                                            both_classes: bool = True) -> str:\n",
    "    \n",
    "    dict_ppl_all_authors = {}\n",
    "    \n",
    "    for author in authors:\n",
    "        if both_classes:\n",
    "            ppl_author_epochs = load_global_perplexities_all_bc(f\"guardian/{dataset}_w{window}_l0/dev\", author, epochs, transcription_type, batch_size)\n",
    "        else:\n",
    "            ppl_author_epochs = load_global_perplexities_all_sc(f\"guardian/{dataset}_w{window}_l0/dev\", author, epochs, transcription_type, batch_size)\n",
    "        author_ep = min(ppl_author_epochs, key=ppl_author_epochs.get)\n",
    "        base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}/{subj}\"\n",
    "        file_name = f\"{subj}_modello_{author}_{transcription_type}_{batch_size}b_{author_ep}ep_global_ppl_score.txt\"\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            ppl = float(f.read().strip())\n",
    "            \n",
    "        dict_ppl_all_authors[author] = ppl\n",
    "    \n",
    "        \n",
    "    # dict_ppl_all_authors is a key = author, value = ppl on the subject -> chose the author with minimum ppl\n",
    "    predicted_author = min(dict_ppl_all_authors, key=dict_ppl_all_authors.get)\n",
    "    return predicted_author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_vs_all_baseline(dataset: str,\n",
    "                            transcription_type: str,\n",
    "                            batch_size: int,\n",
    "                            window: int,\n",
    "                            epochs: list,\n",
    "                            authors: list,\n",
    "                            both_classes: bool) -> None:\n",
    "    base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}\"\n",
    "    \n",
    "    \n",
    "    subjects = sorted([d for d in os.listdir(base_dir_test) if os.path.isdir(os.path.join(base_dir_test, d))])\n",
    "\n",
    "    rows = []\n",
    "    for subj in subjects:\n",
    "        if (subj.split(\"_\")[0] in list_of_author):\n",
    "            pred = all_vs_all_classify_subject_baseline(dataset, window, transcription_type,\n",
    "                                            batch_size, epochs, authors, subj, both_classes)\n",
    "            true_author = subj.split(\"_\")[0] if \"_\" in subj else \"UNKNOWN\"\n",
    "            rows.append({\"subject\": subj,\n",
    "                        \"true_author\": true_author,\n",
    "                        \"pred_author\": pred})\n",
    "\n",
    "    preds_df = pd.DataFrame(rows)\n",
    "    if not preds_df.empty:\n",
    "        preds_df[\"correct\"] = (preds_df[\"true_author\"] == preds_df[\"pred_author\"]).astype(int)\n",
    "        acc = preds_df[\"correct\"].mean()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "        \n",
    "    if (both_classes):\n",
    "        print(f\"🎯 ALL vs ALL Baseline Both Class - Overall accuracy: {acc:.4f}\")\n",
    "    else:  \n",
    "        print(f\"🎯 ALL vs ALL Baseline Same Class - Overall accuracy: {acc:.4f}\")    \n",
    "    \n",
    "    return acc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def run_all_vs_all_baseline(dataset: str,\n",
    "                            transcription_type: str,\n",
    "                            batch_size: int,\n",
    "                            window: int,\n",
    "                            epochs: list,\n",
    "                            authors: list,\n",
    "                            both_classes: bool) -> dict:\n",
    "    base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}\"\n",
    "    \n",
    "    subjects = sorted([d for d in os.listdir(base_dir_test) if os.path.isdir(os.path.join(base_dir_test, d))])\n",
    "\n",
    "    rows = []\n",
    "    for subj in subjects:\n",
    "        if (subj.split(\"_\")[0] in authors):\n",
    "            pred = all_vs_all_classify_subject_baseline(dataset, window, transcription_type,\n",
    "                                                        batch_size, epochs, authors, subj, both_classes)\n",
    "            true_author = subj.split(\"_\")[0] if \"_\" in subj else \"UNKNOWN\"\n",
    "            rows.append({\"subject\": subj,\n",
    "                         \"true_author\": true_author,\n",
    "                         \"pred_author\": pred})\n",
    "\n",
    "    preds_df = pd.DataFrame(rows)\n",
    "\n",
    "    if preds_df.empty:\n",
    "        acc = 0.0\n",
    "        metrics = {\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": 0.0,\n",
    "            \"weighted_f1\": 0.0,\n",
    "            \"per_class\": {}\n",
    "        }\n",
    "        if both_classes:\n",
    "            print(f\"🎯 ALL vs ALL Baseline Both Class - Overall accuracy: {acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"🎯 ALL vs ALL Baseline Same Class - Overall accuracy: {acc:.4f}\")\n",
    "        return metrics\n",
    "\n",
    "    preds_df[\"correct\"] = (preds_df[\"true_author\"] == preds_df[\"pred_author\"]).astype(int)\n",
    "    acc = preds_df[\"correct\"].mean()\n",
    "\n",
    "    # ---- F1 calculations (multi-class, one-vs-rest per class) ----\n",
    "    classes = sorted(preds_df[\"true_author\"].unique().tolist())\n",
    "    per_class = {}\n",
    "    f1s = []\n",
    "    supports = []\n",
    "\n",
    "    for c in classes:\n",
    "        tp = ((preds_df[\"pred_author\"] == c) & (preds_df[\"true_author\"] == c)).sum()\n",
    "        fp = ((preds_df[\"pred_author\"] == c) & (preds_df[\"true_author\"] != c)).sum()\n",
    "        fn = ((preds_df[\"pred_author\"] != c) & (preds_df[\"true_author\"] == c)).sum()\n",
    "        support = (preds_df[\"true_author\"] == c).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "        per_class[c] = {\n",
    "            \"support\": int(support),\n",
    "            \"tp\": int(tp), \"fp\": int(fp), \"fn\": int(fn),\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        }\n",
    "        f1s.append(f1)\n",
    "        supports.append(support)\n",
    "\n",
    "    macro_f1 = float(np.mean(f1s)) if f1s else 0.0\n",
    "    weighted_f1 = float(np.average(f1s, weights=supports)) if sum(supports) > 0 else 0.0\n",
    "    # micro_f1 equals accuracy for single-label multi-class classification\n",
    "    micro_f1 = acc\n",
    "\n",
    "    # print summary\n",
    "    prefix = \"Both Class\" if both_classes else \"Same Class\"\n",
    "    print(f\"🎯 ALL vs ALL Baseline {prefix} - Occuracy (micro F1): {acc:.4f}\")\n",
    "    print(f\"📊 Macro F1: {macro_f1:.4f} | Weighted F1: {weighted_f1:.4f}\")\n",
    "    for c in classes:\n",
    "        pc = per_class[c]\n",
    "        print(f\" • {c}: F1={pc['f1']:.4f} (P={pc['precision']:.4f}, R={pc['recall']:.4f}, support={pc['support']})\")\n",
    "\n",
    "    \n",
    "    metrics_rows = []\n",
    "    for c in classes:\n",
    "        pc = per_class[c]\n",
    "        metrics_rows.append({\n",
    "            \"class\": c,\n",
    "            \"support\": pc[\"support\"],\n",
    "            \"precision\": pc[\"precision\"],\n",
    "            \"recall\": pc[\"recall\"],\n",
    "            \"f1\": pc[\"f1\"],\n",
    "            \"tp\": pc[\"tp\"], \"fp\": pc[\"fp\"], \"fn\": pc[\"fn\"]\n",
    "        })\n",
    "    metrics_rows.append({\"class\": \"macro_avg\", \"support\": int(sum(supports)),\n",
    "                         \"precision\": np.nan, \"recall\": np.nan, \"f1\": macro_f1})\n",
    "    metrics_rows.append({\"class\": \"weighted_avg\", \"support\": int(sum(supports)),\n",
    "                         \"precision\": np.nan, \"recall\": np.nan, \"f1\": weighted_f1})\n",
    "    metrics_rows.append({\"class\": \"micro_avg(=accuracy)\", \"support\": int(len(preds_df)),\n",
    "                         \"precision\": np.nan, \"recall\": np.nan, \"f1\": micro_f1})\n",
    "\n",
    "    # pd.DataFrame(metrics_rows).to_csv(\n",
    "    #     os.path.join(out_dir, f\"all_vs_all_metrics_w{window}_{'bc' if both_classes else 'sc'}.csv\"),\n",
    "    #     index=False\n",
    "    # )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"weighted_f1\": weighted_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"per_class\": per_class\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-shot all-vs-all\n",
    "overall_acc = run_all_vs_all_baseline(dataset, transcription_type, batch_size, window, epochs, list_of_author, both_classes=False)\n",
    "\n",
    "overall_acc = run_all_vs_all_baseline(dataset, transcription_type, batch_size, window, epochs, list_of_author, both_classes=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class tournament delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pairwise_delta_bc(subject: str,\n",
    "                            author_1: str,\n",
    "                            author_2: str,\n",
    "                            epochs: list,\n",
    "                            dataset: str,\n",
    "                            window: int,\n",
    "                            transcription_type: str,\n",
    "                            batch_size: int) -> str:\n",
    "    \"\"\"\n",
    "    Classify a subject between two authors using delta logic:\n",
    "    - find all epoch combinations (e1,e2)\n",
    "    - choose the one that minimizes the separation |ppl1 - ppl2|\n",
    "    - the author with the lower ppl wins\n",
    "    \"\"\"\n",
    "    base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}/{subject}\"\n",
    "    \n",
    "    # check if classification_results_{author_1}_{author_2} exists\n",
    "    if os.path.exists(f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"): \n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "    else:  \n",
    "        results_file = f\"guardian/results/classification_results_{author_2}_{author_1}.csv\"\n",
    "\n",
    "    try:\n",
    "        # Load classification results\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load delta matrix\n",
    "        matrix_file = f\"guardian/matrices/matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return\n",
    "        \n",
    "    min_a1_idx = mean_author_1_df.values.argmin()\n",
    "    min_a2_idx = mean_author_2_df.values.argmin()\n",
    "    min_a1_val = epochs[min_a1_idx]\n",
    "    min_a2_val = epochs[min_a2_idx]\n",
    "\n",
    "    # Filter: only keep configs within optimal epochs\n",
    "    results_df = results_df[\n",
    "        (results_df[f'{author_1}_epoch'] <= min_a1_val) &\n",
    "        (results_df[f'{author_2}_epoch'] <= min_a2_val)\n",
    "    ]\n",
    "    results_df.dropna(subset=['accuracy'], inplace=True)\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(f\"⚠️ No valid configurations found for {author_1} vs {author_2} (delta).\")\n",
    "        return None\n",
    "\n",
    "    # Pick configuration with minimum error bound\n",
    "    if 'delta' in results_df.columns:\n",
    "        min_lower_bound = results_df['delta'].min()\n",
    "        best_config = results_df[results_df['delta'] == min_lower_bound].iloc[0]\n",
    "    else:\n",
    "        # Fallback: choose max accuracy if delta not present\n",
    "        best_config = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "\n",
    "    author_1_epoch = best_config[f'{author_1}_epoch']\n",
    "    author_2_epoch = best_config[f'{author_2}_epoch']\n",
    "    #print(f\"Best config for {subject}: {author_1} epoch={author_1_epoch}, {author_2} epoch={author_2_epoch}\")\n",
    "    \n",
    "    # Load perplexities for the subject at the chosen epochs\n",
    "    file_name = f\"{subject}_modello_{author_1}_{transcription_type}_{batch_size}b_{author_1_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_1 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_1} at epoch {author_1_epoch} for subject {subject}\")\n",
    "    file_name = f\"{subject}_modello_{author_2}_{transcription_type}_{batch_size}b_{author_2_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_2 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_2} at epoch {author_2_epoch} for subject {subject}\")\n",
    "    \n",
    "    if perplexity_author_1 < perplexity_author_2:\n",
    "        winner = author_1\n",
    "    elif perplexity_author_2 < perplexity_author_1:\n",
    "        winner = author_2\n",
    "    else:\n",
    "        winner = None\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pairwise_delta_sc(subject: str,\n",
    "                            author_1: str,\n",
    "                            author_2: str,\n",
    "                            epochs: list,\n",
    "                            dataset: str,\n",
    "                            window: int,\n",
    "                            transcription_type: str,\n",
    "                            batch_size: int) -> str:\n",
    "\n",
    "    base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}/{subject}\"\n",
    "    \n",
    "    # check if classification_results_{author_1}_{author_2} exists\n",
    "    if os.path.exists(f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"): \n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "    else:  \n",
    "        results_file = f\"guardian/results/classification_results_{author_2}_{author_1}.csv\"\n",
    "\n",
    "    try:\n",
    "        # Load classification results\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load delta matrix\n",
    "        matrix_file = f\"guardian/matrices/same_class_matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/same_class_mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/same_class_mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return\n",
    "        \n",
    "    min_a1_idx = mean_author_1_df.values.argmin()\n",
    "    min_a2_idx = mean_author_2_df.values.argmin()\n",
    "    min_a1_val = epochs[min_a1_idx]\n",
    "    min_a2_val = epochs[min_a2_idx]\n",
    "\n",
    "    # Filter: only keep configs within optimal epochs\n",
    "    results_df = results_df[\n",
    "        (results_df[f'{author_1}_epoch'] <= min_a1_val) &\n",
    "        (results_df[f'{author_2}_epoch'] <= min_a2_val)\n",
    "    ]\n",
    "    results_df.dropna(subset=['accuracy'], inplace=True)\n",
    "\n",
    "    if results_df.empty:\n",
    "        print(f\"⚠️ No valid configurations found for {author_1} vs {author_2} (delta).\")\n",
    "        return None\n",
    "\n",
    "    # Pick configuration with minimum error bound\n",
    "    if 'delta' in results_df.columns:\n",
    "        min_lower_bound = results_df['delta'].min()\n",
    "        best_config = results_df[results_df['delta'] == min_lower_bound].iloc[0]\n",
    "    else:\n",
    "        # Fallback: choose max accuracy if delta not present\n",
    "        best_config = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "\n",
    "    author_1_epoch = best_config[f'{author_1}_epoch']\n",
    "    author_2_epoch = best_config[f'{author_2}_epoch']\n",
    "    #print(f\"Best config for {subject}: {author_1} epoch={author_1_epoch}, {author_2} epoch={author_2_epoch}\")\n",
    "    \n",
    "    # Load perplexities for the subject at the chosen epochs\n",
    "    file_name = f\"{subject}_modello_{author_1}_{transcription_type}_{batch_size}b_{author_1_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_1 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_1} at epoch {author_1_epoch} for subject {subject}\")\n",
    "    file_name = f\"{subject}_modello_{author_2}_{transcription_type}_{batch_size}b_{author_2_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_2 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_2} at epoch {author_2_epoch} for subject {subject}\")\n",
    "    \n",
    "    if perplexity_author_1 < perplexity_author_2:\n",
    "        winner = author_1\n",
    "    elif perplexity_author_2 < perplexity_author_1:\n",
    "        winner = author_2\n",
    "    else:\n",
    "        winner = None\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pairwise_baseline_bc(subject: str,\n",
    "                            author_1: str,\n",
    "                            author_2: str,\n",
    "                            epochs: list,\n",
    "                            dataset: str,\n",
    "                            window: int,\n",
    "                            transcription_type: str,\n",
    "                            batch_size: int) -> str:\n",
    "\n",
    "    base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}/{subject}\"\n",
    "    \n",
    "    # check if classification_results_{author_1}_{author_2} exists\n",
    "    if os.path.exists(f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"): \n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "    else:  \n",
    "        results_file = f\"guardian/results/classification_results_{author_2}_{author_1}.csv\"\n",
    "\n",
    "    try:\n",
    "        # Load classification results\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load delta matrix\n",
    "        matrix_file = f\"guardian/matrices/matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return\n",
    "        \n",
    "    # Trova epoche con ppl minima per ciascun autore\n",
    "    author_1_epoch = mean_author_1_df.values.argmin()\n",
    "    author_2_epoch = mean_author_2_df.values.argmin()\n",
    "\n",
    "    #print(f\"Best config for {subject}: {author_1} epoch={author_1_epoch}, {author_2} epoch={author_2_epoch}\")\n",
    "    \n",
    "    # Load perplexities for the subject at the chosen epochs\n",
    "    file_name = f\"{subject}_modello_{author_1}_{transcription_type}_{batch_size}b_{author_1_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_1 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_1} at epoch {author_1_epoch} for subject {subject}\")\n",
    "    file_name = f\"{subject}_modello_{author_2}_{transcription_type}_{batch_size}b_{author_2_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_2 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_2} at epoch {author_2_epoch} for subject {subject}\")\n",
    "    \n",
    "    if perplexity_author_1 < perplexity_author_2:\n",
    "        winner = author_1\n",
    "    elif perplexity_author_2 < perplexity_author_1:\n",
    "        winner = author_2\n",
    "    else:\n",
    "        winner = None\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pairwise_baseline_sc(subject: str,\n",
    "                            author_1: str,\n",
    "                            author_2: str,\n",
    "                            epochs: list,\n",
    "                            dataset: str,\n",
    "                            window: int,\n",
    "                            transcription_type: str,\n",
    "                            batch_size: int) -> str:\n",
    "\n",
    "    base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}/{subject}\"\n",
    "    \n",
    "    # check if classification_results_{author_1}_{author_2} exists\n",
    "    if os.path.exists(f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"): \n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "    else:  \n",
    "        results_file = f\"guardian/results/classification_results_{author_2}_{author_1}.csv\"\n",
    "\n",
    "    try:\n",
    "        # Load classification results\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load delta matrix\n",
    "        matrix_file = f\"guardian/matrices/same_class_matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/same_class_mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/same_class_mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return\n",
    "        \n",
    "    # Trova epoche con ppl minima per ciascun autore\n",
    "    author_1_epoch = mean_author_1_df.values.argmin()\n",
    "    author_2_epoch = mean_author_2_df.values.argmin()\n",
    "\n",
    "    #print(f\"Best config for {subject}: {author_1} epoch={author_1_epoch}, {author_2} epoch={author_2_epoch}\")\n",
    "    \n",
    "    # Load perplexities for the subject at the chosen epochs\n",
    "    file_name = f\"{subject}_modello_{author_1}_{transcription_type}_{batch_size}b_{author_1_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_1 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_1} at epoch {author_1_epoch} for subject {subject}\")\n",
    "    file_name = f\"{subject}_modello_{author_2}_{transcription_type}_{batch_size}b_{author_2_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_2 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_2} at epoch {author_2_epoch} for subject {subject}\")\n",
    "    \n",
    "    if perplexity_author_1 < perplexity_author_2:\n",
    "        winner = author_1\n",
    "    elif perplexity_author_2 < perplexity_author_1:\n",
    "        winner = author_2\n",
    "    else:\n",
    "        winner = None\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pairwise_oracle(subject: str,\n",
    "                            author_1: str,\n",
    "                            author_2: str,\n",
    "                            epochs: list,\n",
    "                            dataset: str,\n",
    "                            window: int,\n",
    "                            transcription_type: str,\n",
    "                            batch_size: int) -> str:\n",
    "\n",
    "    base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}/{subject}\"\n",
    "    \n",
    "    # check if classification_results_{author_1}_{author_2} exists\n",
    "    if os.path.exists(f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"): \n",
    "        results_file = f\"guardian/results/classification_results_{author_1}_{author_2}.csv\"\n",
    "    else:  \n",
    "        results_file = f\"guardian/results/classification_results_{author_2}_{author_1}.csv\"\n",
    "\n",
    "    try:\n",
    "        # Load classification results\n",
    "        results_df = pd.read_csv(results_file)\n",
    "\n",
    "        # Load delta matrix\n",
    "        matrix_file = f\"guardian/matrices/matrix_diff_{author_1}_{author_2}.csv\"\n",
    "        matrix_df = pd.read_csv(matrix_file, index_col=0)\n",
    "        matrix = matrix_df.values\n",
    "\n",
    "        # Load mean perplexities\n",
    "        mean_author_1_file = f\"guardian/matrices/mean_{author_1}.csv\"\n",
    "        mean_author_2_file = f\"guardian/matrices/mean_{author_2}.csv\"\n",
    "\n",
    "        mean_author_1_df = pd.read_csv(mean_author_1_file, index_col=0)\n",
    "        mean_author_2_df = pd.read_csv(mean_author_2_file, index_col=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data for authors {author_1} vs {author_2}: {e}\")\n",
    "        return\n",
    "        \n",
    "    # ordina per accuratezza decrescente\n",
    "    results_df = results_df.sort_values(by='accuracy', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "    # Trova epoche con ppl minima per ciascun autore\n",
    "    author_1_epoch = results_df.iloc[0][f'{author_1}_epoch']\n",
    "    author_2_epoch = results_df.iloc[0][f'{author_2}_epoch']\n",
    "\n",
    "    #print(f\"Best config for {subject}: {author_1} epoch={author_1_epoch}, {author_2} epoch={author_2_epoch}\")\n",
    "    \n",
    "    # Load perplexities for the subject at the chosen epochs\n",
    "    file_name = f\"{subject}_modello_{author_1}_{transcription_type}_{batch_size}b_{author_1_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_1 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_1} at epoch {author_1_epoch} for subject {subject}\")\n",
    "    file_name = f\"{subject}_modello_{author_2}_{transcription_type}_{batch_size}b_{author_2_epoch}ep_global_ppl_score.txt\"\n",
    "    try:\n",
    "        with open(os.path.join(base_dir_test, file_name), 'r') as f:\n",
    "            perplexity_author_2 = float(f.read().strip())\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ PPL file not found for {author_2} at epoch {author_2_epoch} for subject {subject}\")\n",
    "    \n",
    "    if perplexity_author_1 < perplexity_author_2:\n",
    "        winner = author_1\n",
    "    elif perplexity_author_2 < perplexity_author_1:\n",
    "        winner = author_2\n",
    "    else:\n",
    "        winner = None\n",
    "        \n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "def tournament_classify_subject(dataset: str,\n",
    "                                window: int,\n",
    "                                transcription_type: str,\n",
    "                                batch_size: int,\n",
    "                                epochs: list,\n",
    "                                authors: list,\n",
    "                                subject: str,\n",
    "                                strategy: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify a subject with a round-robin 1vs1 tournament using delta logic.\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    author_wins = Counter()\n",
    "\n",
    "    for a1, a2 in itertools.combinations(authors, 2):\n",
    "        \n",
    "        if strategy == 'baseline_sc':\n",
    "            winner = classify_pairwise_baseline_sc(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        elif strategy == 'baseline_bc':\n",
    "            winner = classify_pairwise_baseline_bc(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        elif strategy == 'delta_sc':\n",
    "            winner = classify_pairwise_delta_sc(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        elif strategy == 'delta_bc':\n",
    "            winner = classify_pairwise_delta_bc(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        elif strategy == 'oracle':\n",
    "            winner = classify_pairwise_oracle(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if winner is not None:\n",
    "            author_wins[winner] += 1\n",
    "\n",
    "    if not author_wins:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    max_wins = max(author_wins.values())\n",
    "    top = [a for a, w in author_wins.items() if w == max_wins]\n",
    "        \n",
    "\n",
    "    if len(top) == 1:\n",
    "        return top[0]\n",
    "\n",
    "    # if tie between two authors, do a direct comparison\n",
    "    if len(top) == 2:\n",
    "        a1, a2 = top\n",
    "        \n",
    "        if strategy == 'baseline_sc':\n",
    "            winner = classify_pairwise_baseline_sc(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        elif strategy == 'baseline_bc':\n",
    "            winner = classify_pairwise_baseline_bc(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        elif strategy == 'delta_sc':\n",
    "            winner = classify_pairwise_delta_sc(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        elif strategy == 'delta_bc':\n",
    "            winner = classify_pairwise_delta_bc(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        elif strategy == 'oracle':\n",
    "            winner = classify_pairwise_oracle(subject, a1, a2, epochs,\n",
    "                                         dataset, window, transcription_type, batch_size)\n",
    "        \n",
    "        return winner\n",
    "    \n",
    "    print(\"⚠️ Error: tie between more than two authors, picking one at random\")\n",
    "    return sorted(top)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament_classify_many_subjects(dataset: str,\n",
    "                                            window: int,\n",
    "                                            transcription_type: str,\n",
    "                                            batch_size: int,\n",
    "                                            epochs: list,\n",
    "                                            authors: list,\n",
    "                                            strategy: str) -> tuple[float, pd.DataFrame]:\n",
    "\n",
    "    base_dir_test = f\"dataset/{dataset}_w{window}_l0/test/{transcription_type}\"\n",
    "    subjects = sorted([d for d in os.listdir(base_dir_test) if os.path.isdir(os.path.join(base_dir_test, d))])\n",
    "\n",
    "    rows = []\n",
    "    for subj in subjects:\n",
    "        if (subj.split(\"_\")[0] in list_of_author):\n",
    "            pred = tournament_classify_subject(dataset, window, transcription_type,\n",
    "                                            batch_size, epochs, authors, subj, strategy)\n",
    "            true_author = subj.split(\"_\")[0] if \"_\" in subj else \"UNKNOWN\"\n",
    "            rows.append({\"subject\": subj,\n",
    "                        \"true_author\": true_author,\n",
    "                        \"pred_author\": pred})\n",
    "\n",
    "    preds_df = pd.DataFrame(rows)\n",
    "    if not preds_df.empty:\n",
    "        preds_df[\"correct\"] = (preds_df[\"true_author\"] == preds_df[\"pred_author\"]).astype(int)\n",
    "        acc = preds_df[\"correct\"].mean()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "\n",
    "    #print(f\"✅ Delta BC - Saved tournament predictions: {out_path}\")\n",
    "    print(f\"🎯 {strategy} - Tournament overall accuracy: {acc:.4f}\")\n",
    "\n",
    "    return acc, preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def tournament_classify_many_subjects(dataset: str,\n",
    "                                      window: int,\n",
    "                                      transcription_type: str,\n",
    "                                      batch_size: int,\n",
    "                                      epochs: list,\n",
    "                                      authors: list,\n",
    "                                      strategy: str) -> tuple[float, pd.DataFrame]:\n",
    "   \n",
    "    base_dir_test = f\"guardian/{dataset}_w{window}_l0/test/{transcription_type}\"\n",
    "    subjects = sorted([d for d in os.listdir(base_dir_test) if os.path.isdir(os.path.join(base_dir_test, d))])\n",
    "\n",
    "    rows = []\n",
    "    for subj in subjects:\n",
    "        if (subj.split(\"_\")[0] in authors):  # fixed: was list_of_author\n",
    "            pred = tournament_classify_subject(dataset, window, transcription_type,\n",
    "                                               batch_size, epochs, authors, subj, strategy)\n",
    "            true_author = subj.split(\"_\")[0] if \"_\" in subj else \"UNKNOWN\"\n",
    "            rows.append({\n",
    "                \"subject\": subj,\n",
    "                \"true_author\": true_author,\n",
    "                \"pred_author\": pred\n",
    "            })\n",
    "\n",
    "    preds_df = pd.DataFrame(rows)\n",
    "\n",
    "    if not preds_df.empty:\n",
    "        preds_df[\"correct\"] = (preds_df[\"true_author\"] == preds_df[\"pred_author\"]).astype(int)\n",
    "        acc = preds_df[\"correct\"].mean()\n",
    "    else:\n",
    "        acc = 0.0\n",
    "\n",
    "    # ---- F1 calculations (multi-class one-vs-rest) ----\n",
    "    per_class = {}\n",
    "    macro_f1 = 0.0\n",
    "    weighted_f1 = 0.0\n",
    "    micro_f1 = acc\n",
    "\n",
    "    if not preds_df.empty:\n",
    "        classes = sorted(preds_df[\"true_author\"].unique().tolist())\n",
    "        f1s, supports = [], []\n",
    "\n",
    "        for c in classes:\n",
    "            tp = ((preds_df[\"pred_author\"] == c) & (preds_df[\"true_author\"] == c)).sum()\n",
    "            fp = ((preds_df[\"pred_author\"] == c) & (preds_df[\"true_author\"] != c)).sum()\n",
    "            fn = ((preds_df[\"pred_author\"] != c) & (preds_df[\"true_author\"] == c)).sum()\n",
    "            support = int((preds_df[\"true_author\"] == c).sum())\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "            recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "            f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "            per_class[c] = {\n",
    "                \"support\": support,\n",
    "                \"tp\": int(tp), \"fp\": int(fp), \"fn\": int(fn),\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1\n",
    "            }\n",
    "            f1s.append(f1)\n",
    "            supports.append(support)\n",
    "\n",
    "        macro_f1 = float(np.mean(f1s)) if f1s else 0.0\n",
    "        weighted_f1 = float(np.average(f1s, weights=supports)) if sum(supports) > 0 else 0.0\n",
    "\n",
    "\n",
    "    metrics_rows = []\n",
    "    for c, m in per_class.items():\n",
    "        metrics_rows.append({\n",
    "            \"class\": c,\n",
    "            \"support\": m[\"support\"],\n",
    "            \"precision\": m[\"precision\"],\n",
    "            \"recall\": m[\"recall\"],\n",
    "            \"f1\": m[\"f1\"],\n",
    "            \"tp\": m[\"tp\"], \"fp\": m[\"fp\"], \"fn\": m[\"fn\"]\n",
    "        })\n",
    "    metrics_rows.append({\"class\": \"macro_avg\", \"support\": sum([m[\"support\"] for m in per_class.values()]) if per_class else 0,\n",
    "                         \"precision\": np.nan, \"recall\": np.nan, \"f1\": macro_f1})\n",
    "    metrics_rows.append({\"class\": \"weighted_avg\", \"support\": sum([m[\"support\"] for m in per_class.values()]) if per_class else 0,\n",
    "                         \"precision\": np.nan, \"recall\": np.nan, \"f1\": weighted_f1})\n",
    "    metrics_rows.append({\"class\": \"micro_avg(=accuracy)\", \"support\": int(len(preds_df)),\n",
    "                         \"precision\": np.nan, \"recall\": np.nan, \"f1\": micro_f1})\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows)\n",
    "    # ---- Console summary ----\n",
    "    print(f\"🎯 {strategy} - Tournament overall accuracy (micro F1): {acc:.4f}\")\n",
    "    print(f\"📊 Macro F1: {macro_f1:.4f} | Weighted F1: {weighted_f1:.4f}\")\n",
    "    for c in sorted(per_class.keys()):\n",
    "        m = per_class[c]\n",
    "        print(f\" • {c}: F1={m['f1']:.4f} (P={m['precision']:.4f}, R={m['recall']:.4f}, support={m['support']})\")\n",
    "\n",
    "    return acc, preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\"baseline_sc\", \"baseline_bc\", \"delta_sc\", \"delta_bc\", \"oracle\"]\n",
    "\n",
    "for strategy in strategies:\n",
    "    acc, preds = tournament_classify_many_subjects(dataset, window,\n",
    "                                                     transcription_type,\n",
    "                                                     batch_size, epochs, list_of_author, strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta_recalibration_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
