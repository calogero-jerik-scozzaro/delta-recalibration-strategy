{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles per author:\n",
      "martinkettle: 35\n",
      "simonhoggart: 40\n",
      "catherinebennett: 40\n",
      "hugoyoung: 37\n",
      "royhattersley: 40\n",
      "georgemonbiot: 40\n",
      "willhutton: 40\n",
      "maryriddell: 39\n",
      "peterpreston: 39\n",
      "pollytoynbee: 40\n",
      "jonathanfreedland: 38\n",
      "nickcohen: 40\n",
      "zoewilliams: 40\n",
      "Balancing validation set: 7 texts per author\n",
      "Balancing test set: 7 texts per author\n",
      "Balanced processing complete! Check folder: guardian dataset/guardian_processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "DATA_DIR = Path(\"guardian_dataset/Guardian_extended\")\n",
    "OUTPUT_DIR = Path(\"guardian_dataset/guardian_processed\")\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# COLLECT ALL ARTICLES\n",
    "# -----------------------------\n",
    "author_articles = {}\n",
    "for topic in DATA_DIR.iterdir():\n",
    "    if topic.is_dir():\n",
    "        for author in topic.iterdir():\n",
    "            if author.is_dir():\n",
    "                for article in author.iterdir():\n",
    "                    author_articles.setdefault(author.name, []).append((topic.name, article))\n",
    "\n",
    "# print number of articles per author\n",
    "print(\"Number of articles per author:\")\n",
    "for author, articles in author_articles.items():\n",
    "    print(f\"{author}: {len(articles)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# DETERMINE BALANCED SPLIT COUNTS\n",
    "# -----------------------------\n",
    "min_count = min(len(arts) for arts in author_articles.values())\n",
    "val_count = max(1, int(min_count * VAL_RATIO))\n",
    "test_count = max(1, int(min_count * TEST_RATIO))\n",
    "\n",
    "print(f\"Balancing validation set: {val_count} texts per author\")\n",
    "print(f\"Balancing test set: {test_count} texts per author\")\n",
    "\n",
    "# -----------------------------\n",
    "# CREATE ROOT FOLDERS\n",
    "# -----------------------------\n",
    "for split in [\"train\", \"val\", \"test_with_author\"]:\n",
    "    for author in author_articles:\n",
    "        (OUTPUT_DIR / split / author / \"manual\").mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / \"test\" / \"manual\").mkdir(parents=True, exist_ok=True)  # flat test\n",
    "\n",
    "# -----------------------------\n",
    "# PROCESS AUTHORS\n",
    "# -----------------------------\n",
    "val_labels = []\n",
    "test_labels = []\n",
    "\n",
    "partial_list_authors_20 = [\"nickcohen\",\"zoewilliams\", \"pollytoynbee\",\"peterpreston\", \"royhattersley\", \"simonhoggart\"] \n",
    "\n",
    "for author, articles in author_articles.items():\n",
    "    articles = articles.copy()\n",
    "    random.shuffle(articles)\n",
    "    \n",
    "    # select test and validation\n",
    "    test_files = articles[:test_count]\n",
    "    val_files = articles[test_count:test_count + val_count]\n",
    "    train_files = articles[test_count + val_count:]\n",
    "    \n",
    "    # If author is in partial_list_authors_20 keep only 20% of texts\n",
    "    if author in partial_list_authors_20:\n",
    "        take_n = max(1, int(len(train_files) * 0.2))\n",
    "        train_files = train_files[:take_n]\n",
    "\n",
    "    \n",
    "\n",
    "    # -----------------------------\n",
    "    # TRAIN\n",
    "    # -----------------------------\n",
    "    for topic, file_path in train_files:\n",
    "        out_dir = OUTPUT_DIR / \"train\" / author / \"manual\" / f\"{topic}_{file_path.stem}\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(file_path, out_dir / \"test_text.txt\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # VALIDATION\n",
    "    # -----------------------------\n",
    "    for topic, file_path in val_files:\n",
    "        out_dir = OUTPUT_DIR / \"val\" / author / \"manual\" / f\"{topic}_{file_path.stem}\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        dest_file = out_dir / \"test_text.txt\"\n",
    "        shutil.copy(file_path, dest_file)\n",
    "        val_labels.append({\"filename\": str(dest_file.relative_to(OUTPUT_DIR / \"val\")), \"author\": author})\n",
    "\n",
    "    # -----------------------------\n",
    "    # TEST (author-wise + flat)\n",
    "    # -----------------------------\n",
    "    for topic, file_path in test_files:\n",
    "        # test_with_author\n",
    "        out_dir = OUTPUT_DIR / \"test_with_author\" / author / \"manual\" / f\"{topic}_{file_path.stem}\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        dest_file = out_dir / \"test_text.txt\"\n",
    "        shutil.copy(file_path, dest_file)\n",
    "        test_labels.append({\"filename\": str(dest_file.relative_to(OUTPUT_DIR / \"test_with_author\")), \"author\": author})\n",
    "\n",
    "        # test (flat)\n",
    "        out_dir_flat = OUTPUT_DIR / \"test\" / \"manual\" / f\"{author}_{topic}_{file_path.stem}\"\n",
    "        out_dir_flat.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy(file_path, out_dir_flat / \"test_text.txt\")\n",
    "\n",
    "# -----------------------------\n",
    "# WRITE LABELS.CSV\n",
    "# -----------------------------\n",
    "for split_name, labels in [(\"val\", val_labels), (\"test_with_author\", test_labels)]:\n",
    "    csv_path = OUTPUT_DIR / split_name / \"labels.csv\"\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"filename\", \"author\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(labels)\n",
    "\n",
    "print(\"Balanced processing complete! Check folder:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles per author:\n",
      "martinkettle: 35\n",
      "simonhoggart: 40\n",
      "catherinebennett: 40\n",
      "hugoyoung: 37\n",
      "royhattersley: 40\n",
      "georgemonbiot: 40\n",
      "willhutton: 40\n",
      "maryriddell: 39\n",
      "peterpreston: 39\n",
      "pollytoynbee: 40\n",
      "jonathanfreedland: 38\n",
      "nickcohen: 40\n",
      "zoewilliams: 40\n",
      "Total number of articles: 508\n",
      "Average number of articles per author: 39.08\n",
      "Standard deviation of articles per author: 1.49\n",
      "Number of articles per topic:\n",
      "World: 130\n",
      "Society: 118\n",
      "Politics: 130\n",
      "UK: 130\n",
      "Average number of articles per topic: 127.00\n",
      "Standard deviation of articles per topic: 5.20\n",
      "Average number of words per article: 1038.43\n",
      "Standard deviation of words per article: 371.09\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "DATA_DIR = Path(\"guardian_dataset/Guardian_extended\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# COLLECT ALL ARTICLES\n",
    "# -----------------------------\n",
    "author_articles = {}\n",
    "for topic in DATA_DIR.iterdir():\n",
    "    if topic.is_dir():\n",
    "        for author in topic.iterdir():\n",
    "            if author.is_dir():\n",
    "                for article in author.iterdir():\n",
    "                    author_articles.setdefault(author.name, []).append((topic.name, article))\n",
    "\n",
    "# print number of articles per author\n",
    "print(\"Number of articles per author:\")\n",
    "for author, articles in author_articles.items():\n",
    "    print(f\"{author}: {len(articles)}\")\n",
    "    \n",
    "# print the total number of articles\n",
    "total_articles = sum(len(arts) for arts in author_articles.values())\n",
    "print(f\"Total number of articles: {total_articles}\")\n",
    "    \n",
    "# Print the average number of articles per author and standard deviation\n",
    "import numpy as np\n",
    "article_counts = [len(arts) for arts in author_articles.values()]\n",
    "print(f\"Average number of articles per author: {np.mean(article_counts):.2f}\")\n",
    "print(f\"Standard deviation of articles per author: {np.std(article_counts):.2f}\")\n",
    "\n",
    "# print the average number of articles per topic\n",
    "topic_articles = {}\n",
    "for topic in DATA_DIR.iterdir():\n",
    "    if topic.is_dir():\n",
    "        count = 0\n",
    "        for author in topic.iterdir():\n",
    "            if author.is_dir():\n",
    "                count += len(list(author.iterdir()))\n",
    "        topic_articles[topic.name] = count\n",
    "print(\"Number of articles per topic:\")\n",
    "for topic in topic_articles:\n",
    "    print(f\"{topic}: {topic_articles[topic]}\")\n",
    "print(f\"Average number of articles per topic: {np.mean(list(topic_articles.values())):.2f}\")\n",
    "print(f\"Standard deviation of articles per topic: {np.std(list(topic_articles.values())):.2f}\")\n",
    "\n",
    "# print the average number of words per article\n",
    "word_counts = []\n",
    "for articles in author_articles.values():\n",
    "    for topic, article in articles:\n",
    "        with open(article, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            word_counts.append(len(text.split()))\n",
    "print(f\"Average number of words per article: {np.mean(word_counts):.2f}\")\n",
    "print(f\"Standard deviation of words per article: {np.std(word_counts):.2f}\")\n",
    "\n",
    "# print average word length\n",
    "word_lengths = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles per author:\n",
      "martinkettle: 35\n",
      "simonhoggart: 40\n",
      "catherinebennett: 40\n",
      "hugoyoung: 37\n",
      "royhattersley: 40\n",
      "georgemonbiot: 40\n",
      "willhutton: 40\n",
      "maryriddell: 39\n",
      "peterpreston: 39\n",
      "pollytoynbee: 40\n",
      "jonathanfreedland: 38\n",
      "nickcohen: 40\n",
      "zoewilliams: 40\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "DATA_DIR = Path(\"guardian_dataset/Guardian_extended\")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# COLLECT ALL ARTICLES\n",
    "# -----------------------------\n",
    "author_articles = {}\n",
    "for topic in DATA_DIR.iterdir():\n",
    "    if topic.is_dir():\n",
    "        for author in topic.iterdir():\n",
    "            if author.is_dir():\n",
    "                for article in author.iterdir():\n",
    "                    author_articles.setdefault(author.name, []).append((topic.name, article))\n",
    "\n",
    "# print number of articles per author\n",
    "print(\"Number of articles per author:\")\n",
    "for author, articles in author_articles.items():\n",
    "    print(f\"{author}: {len(articles)}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing author: martinkettle\n",
      "  Total articles: 35\n",
      "  Average tokens per article: 956.40 (±340.27)\n",
      "  Average unique tokens per article: 482.83 (±154.46)\n",
      "  Type-token ratio: 0.5184\n",
      "  Average word length: 4.82 (±2.74)\n",
      "Analyzing author: simonhoggart\n",
      "  Total articles: 40\n",
      "  Average tokens per article: 619.42 (±137.23)\n",
      "  Average unique tokens per article: 380.50 (±65.81)\n",
      "  Type-token ratio: 0.6191\n",
      "  Average word length: 4.70 (±2.57)\n",
      "Analyzing author: catherinebennett\n",
      "  Total articles: 40\n",
      "  Average tokens per article: 1109.30 (±352.78)\n",
      "  Average unique tokens per article: 624.02 (±176.08)\n",
      "  Type-token ratio: 0.5771\n",
      "  Average word length: 5.02 (±2.86)\n",
      "Analyzing author: hugoyoung\n",
      "  Total articles: 37\n",
      "  Average tokens per article: 1145.92 (±123.60)\n",
      "  Average unique tokens per article: 606.30 (±63.84)\n",
      "  Type-token ratio: 0.5303\n",
      "  Average word length: 4.91 (±2.75)\n",
      "Analyzing author: royhattersley\n",
      "  Total articles: 40\n",
      "  Average tokens per article: 898.35 (±255.92)\n",
      "  Average unique tokens per article: 483.10 (±110.58)\n",
      "  Type-token ratio: 0.5432\n",
      "  Average word length: 4.90 (±2.80)\n",
      "Analyzing author: georgemonbiot\n",
      "  Total articles: 40\n",
      "  Average tokens per article: 1059.17 (±216.20)\n",
      "  Average unique tokens per article: 549.67 (±95.99)\n",
      "  Type-token ratio: 0.5244\n",
      "  Average word length: 5.01 (±2.83)\n",
      "Analyzing author: willhutton\n",
      "  Total articles: 40\n",
      "  Average tokens per article: 1117.53 (±148.09)\n",
      "  Average unique tokens per article: 568.62 (±67.40)\n",
      "  Type-token ratio: 0.5114\n",
      "  Average word length: 5.05 (±2.93)\n",
      "Analyzing author: maryriddell\n",
      "  Total articles: 39\n",
      "  Average tokens per article: 1179.33 (±132.87)\n",
      "  Average unique tokens per article: 688.38 (±68.99)\n",
      "  Type-token ratio: 0.5853\n",
      "  Average word length: 5.04 (±2.78)\n",
      "Analyzing author: peterpreston\n",
      "  Total articles: 39\n",
      "  Average tokens per article: 943.85 (±197.54)\n",
      "  Average unique tokens per article: 562.69 (±101.93)\n",
      "  Type-token ratio: 0.6023\n",
      "  Average word length: 4.86 (±2.66)\n",
      "Analyzing author: pollytoynbee\n",
      "  Total articles: 40\n",
      "  Average tokens per article: 1196.42 (±561.98)\n",
      "  Average unique tokens per article: 641.23 (±229.36)\n",
      "  Type-token ratio: 0.5479\n",
      "  Average word length: 4.89 (±2.67)\n",
      "Analyzing author: jonathanfreedland\n",
      "  Total articles: 38\n",
      "  Average tokens per article: 1123.68 (±298.32)\n",
      "  Average unique tokens per article: 598.82 (±126.45)\n",
      "  Type-token ratio: 0.5405\n",
      "  Average word length: 4.81 (±2.65)\n",
      "Analyzing author: nickcohen\n",
      "  Total articles: 40\n",
      "  Average tokens per article: 1274.60 (±345.12)\n",
      "  Average unique tokens per article: 682.12 (±168.31)\n",
      "  Type-token ratio: 0.5451\n",
      "  Average word length: 4.94 (±2.77)\n",
      "Analyzing author: zoewilliams\n",
      "  Total articles: 40\n",
      "  Average tokens per article: 878.80 (±645.42)\n",
      "  Average unique tokens per article: 474.90 (±215.71)\n",
      "  Type-token ratio: 0.5715\n",
      "  Average word length: 4.75 (±2.66)\n",
      "✅ Author-level features saved to author_features.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "total_data = []\n",
    "\n",
    "for author, articles in author_articles.items():\n",
    "    print(f\"Analyzing author: {author}\")\n",
    "    total_articles = len(articles)\n",
    "\n",
    "    # store per-article stats first\n",
    "    article_token_counts = []\n",
    "    article_unique_counts = []\n",
    "    article_type_token_ratios = []\n",
    "    all_tokens = []\n",
    "    all_word_lengths = []\n",
    "\n",
    "    for topic, article in articles:\n",
    "        with open(article, \"r\", encoding=\"utf-8\") as f:\n",
    "            tokens = f.read().split()\n",
    "            article_token_counts.append(len(tokens))\n",
    "            article_unique_counts.append(len(set(tokens)))\n",
    "            all_tokens.extend(tokens)\n",
    "            all_word_lengths.extend([len(t) for t in tokens])\n",
    "            type_token_ratio = len(set(tokens)) / len(tokens) if len(tokens) > 0 else 0\n",
    "            article_type_token_ratios.append(type_token_ratio)\n",
    "\n",
    "    total_tokens = sum(article_token_counts)\n",
    "    total_unique_tokens = len(set(all_tokens))\n",
    "    \n",
    "\n",
    "    # averages and std devs\n",
    "    avg_tokens = np.mean(article_token_counts) if article_token_counts else 0\n",
    "    std_tokens = np.std(article_token_counts) if article_token_counts else 0\n",
    "    avg_unique_tokens = np.mean(article_unique_counts) if article_unique_counts else 0\n",
    "    std_unique_tokens = np.std(article_unique_counts) if article_unique_counts else 0\n",
    "    avg_type_token_ratio = np.mean(article_type_token_ratios) if article_type_token_ratios else 0\n",
    "    std_type_token_ratio = np.std(article_type_token_ratios) if article_type_token_ratios else 0\n",
    "    avg_word_length = np.mean(all_word_lengths) if all_word_lengths else 0\n",
    "    std_word_length = np.std(all_word_lengths) if all_word_lengths else 0\n",
    "\n",
    "    # print results\n",
    "    print(f\"  Total articles: {total_articles}\")\n",
    "    print(f\"  Average tokens per article: {avg_tokens:.2f} (±{std_tokens:.2f})\")\n",
    "    print(f\"  Average unique tokens per article: {avg_unique_tokens:.2f} (±{std_unique_tokens:.2f})\")\n",
    "    print(f\"  Type-token ratio: {avg_type_token_ratio:.4f}\")\n",
    "    print(f\"  Average word length: {avg_word_length:.2f} (±{std_word_length:.2f})\")\n",
    "\n",
    "    # save to list\n",
    "    total_data.append({\n",
    "        \"author\": author,\n",
    "        \"total_articles\": total_articles,\n",
    "        \"avg_tokens\": f\"{avg_tokens:.2f}\",\n",
    "        \"std_tokens\": f\"{std_tokens:.2f}\",\n",
    "        \"avg_unique_tokens\": f\"{avg_unique_tokens:.2f}\",\n",
    "        \"std_unique_tokens\": f\"{std_unique_tokens:.2f}\",\n",
    "        \"avg_type_token_ratio\": f\"{avg_type_token_ratio:.2f}\",\n",
    "        \"std_type_token_ratio\": f\"{std_type_token_ratio:.2f}\",\n",
    "        \"avg_word_length\": f\"{avg_word_length:.2f}\",\n",
    "        \"std_word_length\": f\"{std_word_length:.2f}\"\n",
    "    })\n",
    "\n",
    "# order by author name\n",
    "total_data.sort(key=lambda x: x[\"author\"])\n",
    "\n",
    "# save to CSV\n",
    "with open(\"guardian_dataset/author_features.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=total_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(total_data)\n",
    "\n",
    "print(\"✅ Author-level features saved to author_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "delta_recalibration_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
